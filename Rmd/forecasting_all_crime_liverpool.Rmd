---
title: "Forecasting Crime using Bayesian Spatio-temporal areal unit models"
author: "Thomas Statham"
date: "16/07/2020"
output: html_document
---

Using crime data for 2019, we use the months Jan-Dec to forecast the crime in December at the LSOA level in Liverpool, England. 

```{r, message=FALSE}
# Easy file paths
library(here)
# Fast reading csv
library(vroom)
# Wrangling pkgs
library(tidyverse)
# Spatial pkgs
library(sf); library(spdep);
# API pkgs
library(httr); library(jsonlite); library(RCurl);
# Analysis pkgs
library(INLA); library(brinla); library(INLAOutputs);
```

We first download the lsoa boundary data for Liverpool

```{r}
# Define four features of API
endpoint = 'https://ons-inspire.esriuk.com/arcgis/rest/services/'
boundary_lyr = 'Census_Boundaries/Lower_Super_Output_Areas_December_2011_Boundaries/'
resolution = 'MapServer/2/'
query = 'query?where=UPPER(lsoa11nm)%20like%20%27%25LIVERPOOL%25%27&outFields=*&outSR=4326&f=json'
# Download liverpool lsoas through API
liv = st_read(str_c(endpoint,boundary_lyr,resolution,query,sep='/'))
# Transform to British National Grid to simplify object & to use later
liv_bng = st_transform(liv,crs=27700) %>%
  st_simplify(dTolerance = 0.05) %>% # Simplify object
  st_cast("MULTIPOLYGON") %>% # Cast as multipoly
  select(lsoa11cd, lsoa11nm) # Subset lsoa11cd/geometry only
# Transform back to 4326 - ins is because the
liv = st_transform(liv_bng,crs=4326)
# Drop geometry for merging later
liv_null = liv %>%
  st_drop_geometry()
# Plot with red because Liverpool FC
plot(st_geometry(liv), col='red')
```

Get crime (target) data from Police API.

```{r, message=FALSE}
source(here('R/crime_analysis_helper.R'))
# Define date range
date = seq(as.Date('2019/01/01'), by = 'month', length.out = 12) %>% 
  str_sub(end=-4)
# Define dates without days
dates = date %>%
as.list() # Convert to list
# Iterate through list of dates and apply function
crime_2019 = lapply(dates, crime_api, poly=liv)
```

Aggregate crime point data to lsoa level.

```{r}
# Row bind list of df to single df
crime_2019 = do.call(rbind, crime_2019)
# Convert to sf object
crime_2019 = st_as_sf(crime_2019, coords = c('lng', 'lat'),crs = 4326) %>% # Make spatial
  st_transform(crs=27700) # Reproject
# Add crime count data
crime_2019$count = 1
# Merge crime api data with boundary
crime_2019 = st_join(liv_bng, crime_2019, join = st_intersects) %>% # point in poly
  na.omit() %>% # Drop no lsoa matches
  group_by(lsoa11cd, lsoa11nm, month) %>% # Group by
  summarise(count = sum(count)) %>%  # Summarise by count
  arrange(month) # Order by ascending
```
## Add 0 values to months with no data
We first generate dummy data to insert 0 values into lsoas which have no data for each time period. We then merge the dummy dataset back to the actual dataset and replace null values with 0.

```{r}
# Create dataframe based on range of months and repeat by 
df = data.frame(list(month=date)) %>%  
  slice(rep(1:n(), each = length(liv$lsoa11cd)))

# Repeat time stamps by n msoas
df1 = df %>%  
  slice(rep(1:n(), each = length(liv$lsoa11cd)))
# Repeat msoas by n months
df1$count1 = 1
# Repeat msoas by n months
df2 = do.call("rbind", replicate(length(df$month), liv_null, simplify = FALSE)) %>% 
  bind_cols(df1) %>%  
  group_by(lsoa11cd, month) %>%
  summarize(n())
  
# Merge dummy dataset with actual dataset and replace null values with 0
crime_2019 = crime_2019 %>%
  full_join(df2, by=c('lsoa11cd', 'month')) %>%
  mutate(count=replace_na(count,0)) %>%
  select(-'n()')
```
## Load and process predictors from csv

```{r, message=FALSE}
iuc = vroom(here('/data/iuc2018.csv'), col_select = ends_with(c('LSOA11_CD','GRP_CD'))) %>%
rename(lsoa11cd = LSOA11_CD, iuc_group = GRP_CD) %>% # Rename cols
inner_join(liv_null, by='lsoa11cd') %>% # Inner join lookup table
select(lsoa11cd, iuc_group) #%>% # Subset columns

imd = vroom(here('/data/imd2015.csv'), col_select = c(1, 5, 8, 14)) %>%
rename(lsoa11cd = 1, imd_score=2, income_score=3, education_score=4) %>% # Rename cols
inner_join(liv_null, by='lsoa11cd') %>% # Inner join lookup table
select(lsoa11cd, imd_score, income_score, education_score) #%>% # Subset columns
```

## Using the [Stat-Explore-API](https://stat-xplore.dwp.gov.uk/webapi/online-help/Open-Data-API.html) we will download housing benefits data. 
Note: You will need to obtain the API key.

```{r, message=FALSE}
# Define unique lsoa
lsoa = unique(liv$lsoa11cd)
# Subset first 2 items
lsoa = str_sub(lsoa, start=3)

# Ask for API key
api_key = rstudioapi::askForPassword()
# Define url
url = 'https://stat-xplore.dwp.gov.uk/webapi/rest/v1/table'

# Define query
query = list(database = unbox('str:database:hb_new'),
measures = 'str:count:hb_new:V_F_HB_NEW',
dimensions = c('str:field:hb_new:V_F_HB_NEW:COA_CODE',
'str:field:hb_new:F_HB_NEW_DATE:NEW_DATE_NAME') %>% matrix(),
recodes = list(
`str:field:hb_new:V_F_HB_NEW:COA_CODE` = list(
map = as.list(paste0('str:value:hb_new:V_F_HB_NEW:COA_CODE:V_C_MASTERGEOG11_LSOA_TO_MSOA:E0', lsoa))),
`str:field:hb_new:F_HB_NEW_DATE:NEW_DATE_NAME` = list(
map = list('str:value:hb_new:F_HB_NEW_DATE:NEW_DATE_NAME:C_HB_NEW_DATE:201906'))
)) %>% toJSON()

# Define request
request = POST(
url=url,
body=query,
config=add_headers(APIKey=api_key),
encode='json')

# Define response
response = fromJSON(content(request, as='text'), flatten=TRUE)
# Extrast list items and convert to a dataframe
dimnames = response$fields$items %>%
  map(~.$labels %>% unlist)
values = response$cubes[[1]]$values
dimnames(values) = dimnames
# Convert to DataFrame
house_benefits = as.data.frame.table(values, stringsAsFactors=FALSE) %>%
as_tibble() %>%
set_names(c(response$fields$label,'value')) %>%
rename(lsoa11nm=1, benefits=value) %>%
select(-Month)
```

## Next we merge all of our predictors with the target variable.

```{r}
# This is just to test the merge works
crime_2019_sub = crime_2019 %>%
group_by(lsoa11cd, lsoa11nm) %>%
summarise(count = sum(count))

# Merge all variables with the broadband faults
liv_crime= crime_2019_sub %>%
  inner_join(house_benefits, by='lsoa11nm') %>%
  inner_join(iuc, by='lsoa11cd') %>%
  inner_join(imd, by='lsoa11cd') %>%
  select(count, benefits, iuc_group, imd_score, education_score) %>% # Subset cols
  st_drop_geometry() # drop geom
```

## Check the correlation of predictors with target variables. 

```{r}
library(lares)
lares::corr_var(liv_crime, # name of dataset
  count, # name of variable to focus on
  top = 5 # display top 5 correlations
)
```

## Create adjacency matrix for specifying the spatial relationship between lsoas

```{r}
coords = st_coordinates(st_centroid(st_geometry(liv_bng)))
xx = poly2nb(as(liv_bng, 'Spatial'))
plot(xx, coords)
# Create binary  matrix by specifying style="B" & zero policy ensures matrix is complete even if there are no neighbours
adj = nb2mat(xx, style="B", zero.policy=TRUE)
# Define as spare matrix
adj = as(adj, "dgTMatrix")
# 
nb2INLA("map.adj", xx)
g = inla.read.graph(filename = "map.adj")
```

## Generate nas in data

```{r}
# Generate data for forecasting December crime
df = crime_2019 %>%
  # Drop geoms
  st_drop_geometry() %>% 
  # Select data
  select(lsoa11cd, month, count) %>% 
  # Input na
  mutate(month=ifelse(month=='2020-12',NA, month))
# Create copy of month for iid term
df$month1 = df$month
# Convert geography to numeric because inla requires numeric values 
df$lsoa11cd1 =  as.numeric(as.factor(rep(df$lsoa11cd)))
```

## Specify space-time forecasting model


### Conditional Auto Regressive (CAR) models
When working with spatial data, it is reasonable to assume that observations in neighboring areas may be more or less alike simply due to their proximity, and hence exhibit autocorrelation47. We confirm this by first running a Moran’s I test, which measures whether spatial autocorrelation is present in the data. Due to this autocorrelation, we cannot run a simple linear regression analysis, as spatial dependencies would exist in the error term. Hence, we run our analysis using a conditional autoregressive prior (CAR), as initially proposed by Besag and colleagues51,52, which captures spatial dependence between neighbors through an adjacency matrix of the areal units.

The CAR model quantifies the spatial relationship in the data by including a conditional distribution in the error term for area i, ei. The conditional distribution of ei is thus represented as:

$$e_j \sim i \sim N \Bigg(\sum_{j\sim i} \frac{c_{ij}e_j}{\sum_{j \sim i} c_{ij}}, \frac{\sigma^2_{ei} }{\sum_{j \sim i} c_{ij}} \Bigg)$$

where ej~i is the e–i vector including only neighboring areas of i; e–i is the vector of all the errors terms except for e–i itself; and cij are dependence parameters used to represent the spatial dependence between the area.

## AutoRegressive 1 model
...

```{r}
st_model = function(data, graph){
  st_model = count ~ 1 +
  #f(lsoa11cd1, model='bym', graph=graph) + # modellng structured + unstructured spatial components
  f(month,model='ar1') + # temporal structured component
  f(month1,model='iid')  # temporal unstructured component
  
  inla_output = inla(st_model,family='poisson',data=data, 
                        control.compute=list(dic=TRUE,config=TRUE),
                        control.predictor=list(link=1))
  return(inla_output)
}

dec_pred = st_model(data=df, graph=g)
summary(dec_pred)
```

